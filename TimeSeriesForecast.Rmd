---
title: "PES University"
subtitle: "**UE21CS342AA2-Data Analytics**\n\n**Worksheet 3 Part 1**\n\n"
author: 
output: pdf_document
date: "2023-10-20"
urlcolor: blue
editor_options:
  markdown:
    wrap: 72
---

```{r }
knitr::opts_chunk$set(echo = TRUE)
```

## Student Details

-   Name: 

-   SRN:

-   Section:J

\-\-\-\--

# Water Level Forecasting

Imagine that you're working towards water conservation.

You have been given a dataset, and your task is to predict the level of
water, to facilitate governance decisions such as reservoir design,
water drainage policies, etc.

### Contents of the worksheet

It is suggested to have a grip on the theoretical concepts of:

-   Components of time series data

-   Decomposition of time series data

-   Exponential Smoothing techniques

-   Stationary Signals, Dickey-fuller test and Differencing

-   Forecasting with AR, MA, ARMA

-   Autocorrelation (ACF, PACF) and ARIMA

-   Exogenous variables in time series (ARIMAX and SARIMAX)

\-\-\-\--

### Dataset

The data is provided to you in the `water.csv` file.

The data dictionary is as follows:

    date - The date of collection of data

    rainfall - Measurement of rain falling in the area (in mm)

    groundwater_level - Indicates groundwater level, expressed in
    ground level 
    (meters from the ground floor)

    temperature - Indicates temperature (in Celsius) in the area

    drainage_volume - Volume of water taken from the groundwater storage for usage purposes

    river_level - Indicates the river level (in m) which feeds the groundwater indirectly

The **target variable** is `groundwater_level`, which is what we shall
forecast in this worksheet.

`groundwater_level` is quite important in hydrogeology, where it is used
for water resource management, drought and flood mitigation, design and
maintenance of groundwater storage systems, etc.

\-\-\-\--

### Data Ingestion and Preprocessing

-   Reading this file into a data.frame object

```{r}
df <- read.csv('water.csv',header=TRUE)
head(df)
```

-   We'll pick the `groundwater_level` column in our DataFrame, since
    we'll be doing most of our analysis on this variable.

```{r}
gwl <- df$groundwater_level
head(gwl)
```

# Univariate Time Series Modelling

-   While dealing with Univariate Modeling, we consider only the date
    column and the target column.

> **Tip:** **Make sure to check that the time column is in chronological
> order, and the time intervals are equidistant!**

-   We need to create a `ts` object in R. This can be done using the
    `ts` function. We'll use our frequency as 365, since we have daily
    data.

-   If we were using weekly data, we'd have the frequency as 52, for
    monthly data it'd be 12, and so on.

```{r}
gwl_ts <- ts(gwl, frequency=365, start=c(2015, 1, 9))
gwl_ts[1:90]
```

-   Visualize the Time-Series of `groundwater_level` column

```{r fig.width=5, fig.height=3}
plot.ts(gwl_ts)
```

## Section 1: Decomposition

### *Question 1.1:* Decompose the `groundwater_level` column into the constituent components, and plot them.

> Hint: Look at the `decompose` function.
############################

```{r}
decomposed_data <- decompose(gwl_ts)
plot(decomposed_data)

```
###########################
> **Sometimes, we look at upsampling or downsampling the data. For
> instance, if we have sensor data for each second, we might not need
> such granular data, and we downsample the data to daily data or hourly
> data or so.**
>
> **Explore further here:
> <https://machinelearningmastery.com/resample-interpolate-time-series-data-python/>**

You can also explore adding the decomposed versions of each feature
(column) to your data, and utilize it as exogenous variables for
multivariate forecasting! This would require you to decompose all
features, such as `temperature`, `rainfall`, etc. as well, which is out
of scope of this worksheet.

</div>

### *Question 1.2 :* Which model of time series did you use for decomposition, and why? (between additive and multiplicative models)

An additive model is appropriate when the seasonal component of a time series remains constant over time.

\-\-\-\--

## Section 2: Exponential Smoothing

-   Perform forecasts using Single, Double and Triple Exponential
    Smoothing.

-   Plot forecasts of all three forecasts (using different colors),
    against the true values. (Use `lines`)

-   Only one function needed for all three forecasts, only requiring you
    to change the parameters to get each of the 3 models.

-   Hint: look at the `HoltWinters` function in R

-   Go ahead, and experiment with the values of `alpha`, `beta` and
    `gamma` and see how the forecast changes.

```{=html}
<!-- -->
```
    ####################################################
```{r}
# Single Exponential Smoothing
single_smoothing <- HoltWinters(gwl_ts, alpha=0.2, beta=FALSE, gamma=FALSE)

# Double Exponential Smoothing
double_smoothing <- HoltWinters(gwl_ts, alpha=0.2, beta=0.2, gamma=FALSE)

# Triple Exponential Smoothing
triple_smoothing <- HoltWinters(gwl_ts, alpha=0.2, beta=0.2, gamma=0.2)

# Truncate the forecasts to match the length of the test data
forecast_length <- length(fitted(single_smoothing))

# Plot the forecasts
plot(gwl_ts, col='black', type='l', ylab='Groundwater Level')
lines(fitted(single_smoothing)[1:forecast_length], col='blue')
lines(fitted(double_smoothing)[1:forecast_length], col='red')
lines(fitted(triple_smoothing)[1:forecast_length], col='green')
```

    ####################################################

### Question 2.1: Compare accuracy metrics (MAE, MAPE, MSE, RMSE) of the three models with the original series.

```{r}

calculate_metrics <- function(actual_values, predicted_values) {

  mae_value <- mean(abs(actual_values - predicted_values))
  

  mape_value <- mean(abs((actual_values - predicted_values) / actual_values)) * 100
  

  mse_value <- mean((actual_values - predicted_values)^2)
  

  rmse_value <- sqrt(mse_value)
  
  return(list(MAE = mae_value, MAPE = mape_value, MSE = mse_value, RMSE = rmse_value))
}
```

### Use the above function on your results from the three types of exponential smoothing and analyze.

    ####################################################
```{r}
actual_values <- as.numeric(gwl_ts)
predicted_single <- fitted(single_smoothing)
predicted_double <- fitted(double_smoothing)
predicted_triple <- fitted(triple_smoothing)

metrics_single <- calculate_metrics(actual_values, predicted_single)
metrics_double <- calculate_metrics(actual_values, predicted_double)
metrics_triple <- calculate_metrics(actual_values, predicted_triple)

# Compare the metrics
metrics_single
metrics_double
metrics_triple

```

    ####################################################

\-\--

## Section 3: Stationarity

-   **Testing for stationarity**

    -   The Augmented Dickey-Fuller test (ADF) can be employed to assess the stationarity of a time series.

### *Question 3.1:* What are the null hypothesis and alternate hypothesis in this case?

Null Hypothesis (H0): The null hypothesis posits that the time series data is non-stationary, meaning it has a unit root, and it exhibits a structure that is not consistent over time. In other words, it suggests that the data has a stochastic or trending component.

Alternative Hypothesis (H1): The alternative hypothesis, on the other hand, asserts that the time series data is stationary, which means it lacks a unit root, and its statistical properties do not change over time. This indicates that the data follows a stationary or more predictable pattern.

    ####################################################
```{r}
# Load the tseries package
# install.packages("tseries")

library(tseries)

# Calculate the differenced series
gwl_diff <- diff(gwl_ts)

# Perform the Augmented Dickey-Fuller (ADF) test
adf_test_diff <- adf.test(gwl_diff)

# Print the ADF test results
print(adf_test_diff)

```

    ####################################################

```{r}
library(tseries)

adf_test <- adf.test(gwl_ts)
print(adf_test)
```

### *Question 3.2:* What can you tell from the adf-test? Is this series stationary or non-stationary? Why do you say so?

    ####################################################
With a p-value greater than 0.05, we fail to reject the null hypothesis, indicating that the series is non-stationary.

    ####################################################

<div>

> -   If the data is not stationary, and if we intend to use a model
>     like ARIMA, the data has to be transformed.
>
> -   Two most common methods to transform series to stationary are:
>
>     -   **Transformations:** eg. log or square root or combinations of
>         these transformations to stabilize non-constant variance.
>
>     -   **Differencing:** subtract current value from previous (with a
>         certain degree)
>
> Check this out for more information, and an implementation in Python!:
>
> <https://www.kaggle.com/code/rdizzl3/time-series-transformations>

</div>

### *Question 3.3:* Create a new dataframe using suitable differencing order, to convert the data to stationary time series.

> **Hint:** You can use the ADF function to confirm the time series is
> stationary after transformation.

    ####################################################
```{r}
gwl_diff <- diff(gwl_ts)
adf_test_diff <- adf.test(gwl_diff)
print(adf_test_diff)

```

    ####################################################

\-\-\-\--

## Section 4: Autocorrelation Analysis

-   We will experiment and plot two functions:

    -   **ACF (Autocorrelation function):** The autocorrelation function
        (ACF) is a statistical technique that we can use to identify how
        correlated the values in a time series are with each other. The
        ACF plots the correlation coefficient against the lag, which is
        measured in terms of a number of periods or units.

    -   **PACF (Partial Autocorrelation function):** Partial
        autocorrelation is a statistical measure that captures the
        correlation between two variables after controlling for the
        effects of other variables.

```{r fig.width=5, fig.height=4}
library(tseries)

 acf_result <- acf(gwl_diff, lag.max = 50, plot = TRUE)
 pacf_result <- pacf(gwl_diff, lag.max = 50, plot = TRUE)
```

### *Question 4.1 :* What are the values of p, q and d? How did you come to this conclusion, looking at the ACF, PACF plots?

> Hint: The value of `d` is decided by the order of differencing, as
> transformed in the previous section.

    ####################################################
The selection of p, q, and d for ARIMA modeling is based on analysis of the ACF and PACF plots. The value of p, representing the lag for the autoregressive term, is determined by locating where the PACF plot terminates. The value of q, which signifies the lag for the moving average term, is identified by the point where the ACF plot ceases. Finally, the value of d, representing the order of differencing, corresponds to the number of differencing operations required to render the time series stationary.

    ####################################################

\-\-\-\--

## Section 5: Time Series Forecasting using Statistical Models

-   Before we apply models for forecasting, we need to create a training
    and validation/test set, as would be the procedure for most machine
    learning problems.

-   However, one thing to keep in mind while performing this split for
    time series data: ***NEVER*** **perform a random split.**

### *Question 5.1:* Why do you think we shouldn't perform a random split on our data to create a train/test/dev set?

    ####################################################
Randomly splitting time series data is discouraged since it disrupts the temporal sequence of the data. Given the inherent sequential nature of time series data, where past observations can influence future ones, it is more suitable to perform a chronological order split.

    ####################################################

-   We shall go ahead and split according to chronological order here.

-   We implement a 80-20% split for train-test sets here. Ideally, you
    would also have a validation set, but it isn't necessary within this
    worksheet.

```{r}
split_index <- floor(length(gwl_ts) * 0.8) 

train <- ts(gwl_ts[1:split_index])
test <- ts(gwl_ts[(split_index + 1):length(gwl_ts)], start=split_index+1) 
```

<div>

> If you're working with Python, `scikit-learn` has a function which is
> capable of creating train-test-validation splits for time series data
> automatically.

</div>

### *Question 5.2:* Implement AR, MA and ARMA models, with the optimal values of `p` and `q` as calculated from PACF and ACF plots previously.

```{r}
 # install.packages("forecast")
library(forecast)
```

> **Hint:** Look at `Arima` function in R.

    ####################################################
```{r}
p<- 0.48
q <-0.05
# AR Model
ar_model <- Arima(train, order=c(p, 0, 0))

# MA Model
ma_model <- Arima(train, order=c(0, 0, q))

# ARMA Model
arma_model <- Arima(train, order=c(p, 0, q))

```

    ####################################################

### *Question 5.3*: Implement the ARIMA model, with the optimal values of `p`, `d`, `q` as calculated from PACF and ACF plots previously.

    ####################################################
```{r}
d<-1
arima_model <- Arima(train, order=c(p, d, q))

```

    ####################################################

### *Question 5.4:*

### 1. Which models performed better? The exponential smoothing models, or the statistical models (AR, MA, ARMA, ARIMA). Why?

### 2. Is this always the case?

### 3. Do you think you'd get a better result if you used SARIMA?

### 4. Do you think exogenous variables would give you a better accuracy? (i.e ARIMAX?)

### 5. Other than providing the other features in the dataset (such as `temperature`, `rainfall`, etc.), what kind of engineered features would you give as exogenous variables that could improve performance?

> Hint: Some of these possible features were mentioned previously in
> this worksheet.

    ####################################################
1. The performance of exponential smoothing and statistical models may vary depending on the dataset. The selection between them should be based on their specific performance in the given dataset.

2. It's not always the case that one model type outperforms the other. The choice hinges on the dataset's characteristics and the unique forecasting requirements.

3. SARIMA has the potential to enhance forecasting accuracy, particularly when the time series exhibits seasonality. Exploring SARIMA models and fine-tuning the appropriate hyperparameters is advisable.

4. Incorporating exogenous variables, such as temperature and rainfall, has the potential to enhance forecasting accuracy. ARIMAX models are well-suited for this purpose.

5. Exogenous variables related to weather, seasonal patterns, and historical trends can serve as additional features to boost model performance. Consider also incorporating lagged values from other time series in the dataset as exogenous variables. Additional feature engineering may be necessary to identify pertinent variables.

    ####################################################

\-\-\-\-\--

### Congratulations on reaching the end of this worksheet! I hope you enjoyed it, and have an understanding of how practical time series analysis works.

Some advanced concepts for you to explore are listed below:

-   One of the main errors of dealing with time-series data includes
    preventing `lookahead`. It's extremely important that you aren't
    looking at future values to predict earlier ones. You can read more
    about it here:

    <https://bowtiedraptor.substack.com/p/look-ahead-bias-and-how-to-prevent>

-   Although the dataset provided to you for this worksheet was cleaned
    prior, real world data is extremely dirty. Time series data
    especially tends to contain quite a few missing values. Try to
    explore some ways of taking care of missing values in data. Some
    techniques include imputation, forward fills, interpolation, moving
    averages, etc.

-   Understanding some Classical Machine Learning techniques for Time
    Series Forecasting, such as Decision Trees, Forests, Feed-forward
    Neural Networks, etc.

    <https://machinelearningmastery.com/random-forest-for-time-series-forecasting/>

    <https://www.section.io/engineering-education/feedforward-and-recurrent-neural-networks-python-implementation/>

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--